# PPO

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250605111503293.png" alt="image-20250605111503293" style="zoom: 33%;" />

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250605112101218.png" alt="image-20250605112101218" style="zoom:33%;" />

按照Return结果对神经网络参数求导，进行梯度上升，即对Return>0的所有Trajectory采取对应action的概率（采样概率）

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250605112456968.png" alt="image-20250605112456968" style="zoom: 33%;" />

on policy策略表示用于batch数据采集与进行训练的网络是同一个，在同一个环境与参数下运行一个batch的数据用于参数更新

理论上一个action只会影响后续产生的reward而不完全影响整个Trajectory的reward，且影响存在衰减

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250605113751469.png" alt="image-20250605113751469" style="zoom:33%;" />

在原本的梯度公式上改进：只计算该action之后的reward，给action对应reward加上一个衰减因子

由于存在优势局面（不管如何操作return都是大于0）加上critic给出baseline，更加具体地判断action好坏，加快训练速度

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250605114900796.png" alt="image-20250605114900796" style="zoom:33%;" />

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250605115152263.png" alt="image-20250605115152263" style="zoom: 33%;" />

动作价值函数Q表示特定动作的优势，状态价值函数A衡量状态的价值

动作价值函数Q可以通过贝尔曼公式由状态价值函数A表示，以此进行类推，实际采样越多方差越大、偏差越小，函数估计则相反

Q就是在状态 st 下，如果我刻意选择了动作 at 而不是随机按策略走，能多赚（或少赚）多少回报

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250605140436060.png" alt="image-20250605140436060" style="zoom: 50%;" />

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250605140504368.png" alt="image-20250605140504368" style="zoom:50%;" />

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250605141248915.png" alt="image-20250605141248915" style="zoom:50%;" />

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250605141416734.png" alt="image-20250605141416734" style="zoom:50%;" />

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250605141445464.png" alt="image-20250605141445464" style="zoom:50%;" />

### GAE**（Generalized Advantage Estimation，广义优势估计）**

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250605115641507.png" alt="image-20250605115641507" style="zoom:33%;" />

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250605115807718.png" alt="image-20250605115807718" style="zoom: 33%;" />

GAE优势动作估计方法对后续的所有步骤进行采样，并为其加上衰减因子，能够平衡TD与蒙特卡罗方法的偏差与方差，实际截断后续采样

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250605144712763.png" alt="image-20250605144712763" style="zoom: 50%;" />

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250605144731977.png" alt="image-20250605144731977" style="zoom:50%;" />

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250605144928938.png" alt="image-20250605144928938" style="zoom:50%;" />

将状态估计函数V作为一个神经网络，可以与策略函数A共用网络参数，A接softmax，V只接一个单独的输出值为当前状态价值

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250605155030864.png" alt="image-20250605155030864" style="zoom:50%;" />

on policy策略每采集一次数据模型就要更新一次参数，数据只使用一次，off policy策略采集的数据则可以重复使用多次

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250605155359414.png" alt="image-20250605155359414" style="zoom: 50%;" />

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250605161125065.png" alt="image-20250605161125065" style="zoom:50%;" />

重要性采样即如果希望得到在新分布p上的期望只需要求旧分布q的期望，再乘以p/q的分布差异（重要性权重）

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250605155534597.png" alt="image-20250605155534597" style="zoom:50%;" />

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250605160532166.png" alt="image-20250605160532166" style="zoom: 50%;" />

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250605164434021.png" alt="image-20250605164434021" style="zoom:50%;" />

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250605155706492.png" alt="image-20250605155706492" style="zoom:50%;" />

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250605160622803.png" alt="image-20250605160622803" style="zoom:50%;" />

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250605160851570.png" alt="image-20250605160851570" style="zoom: 50%;" />

# RLHF + PPO

![image-20250605181718954](C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250605181718954.png)

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250605181811948.png" alt="image-20250605181811948" style="zoom:50%;" />

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250605181837088.png" alt="image-20250605181837088" style="zoom:50%;" />

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250605181934326.png" alt="image-20250605181934326" style="zoom:50%;" />

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250605181950162.png" alt="image-20250605181950162" style="zoom:50%;" />

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250605182016910.png" alt="image-20250605182016910" style="zoom: 50%;" />

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250605182206456.png" alt="image-20250605182206456" style="zoom:50%;" />

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250605182314030.png" alt="image-20250605182314030" style="zoom:50%;" />

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250605182431638.png" alt="image-20250605182431638" style="zoom:50%;" />

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250605182554654.png" alt="image-20250605182554654" style="zoom:50%;" />

# MOE

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250606014611883.png" alt="image-20250606014611883" style="zoom:50%;" />

路由网络依据输入选择排名靠前的几个专家对输入进行处理