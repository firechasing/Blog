## Vision Transformer

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250525220813533.png" alt="image-20250525220813533" style="zoom: 33%;" />

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250525220858389.png" alt="image-20250525220858389" style="zoom:33%;" />

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250525221532121.png" alt="image-20250525221532121" style="zoom: 50%;" />

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250525221557181.png" alt="image-20250525221557181" style="zoom:50%;" />

对于自监督学习，BERT模型采用掩码（MLM）方式屏蔽部分字符进行完形填空，GPT模型进行自回归预测下一个字符

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250527105950340.png" alt="image-20250527105950340" style="zoom:50%;" />

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250527111637027.png" alt="image-20250527111637027" style="zoom:50%;" />

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250527111723043.png" alt="image-20250527111723043" style="zoom:50%;" />

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250527111902918.png" alt="image-20250527111902918" style="zoom:50%;" />

0号字符cls自身不包含信息，在进行自注意力后汇聚所有patch的信息，据其对图像进行分类

输入图像224×224×3，分割为16×16×3(向量长度)×196(token数)，经过768×768的全连接得到潜向量196(token数)×768(向量维度)，额外加上1×768的初始cls向量，共197×768（对此向量序列加上对应值进行位置编码），划分为197×64×12共12个注意力头，输出矩阵仍旧是197×768，可以在相同结构上无限堆叠

CNN有平移不变性与局部不变性的归纳偏置（先验知识），需求的数据量较少，VIT没有归纳偏置，需求数据量更大但上限更高，先用CNN提取特征再用VIIT能够在样本较小时也取得较好的

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250527115002688.png" alt="image-20250527115002688" style="zoom:50%;" />

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250527115029876.png" alt="image-20250527115029876" style="zoom:50%;" />

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250527115046480.png" alt="image-20250527115046480" style="zoom:50%;" />

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250527115113732.png" alt="image-20250527115113732" style="zoom:50%;" />

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250527115214662.png" alt="image-20250527115214662" style="zoom:50%;" />

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250527115134945.png" alt="image-20250527115134945" style="zoom:50%;" />

# MAE（Masked Autoencoder）

MAE尝试将BERT的自监督（填空）机制应用到CV任务上

atuoz自表示输入样本与输出有同一个来源

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250527150820548.png" alt="image-20250527150820548" style="zoom: 33%;" />

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250527151145467.png" alt="image-20250527151145467" style="zoom:50%;" />

MAE结构将输入patch的一部分掩盖，只输入未掩盖部分，在潜空间内对掩盖部分进行预测，解码后还原为完整patch

关键在于采用非对称结构的编码与解码器 + 覆盖住绝大部分图像以提升表征能力 + 训练策略超参数调节

大部分图像被覆盖仍能还原说明图像信息存在大量的冗余

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250527154204835.png" alt="image-20250527154204835" style="zoom: 50%;" />

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250527154229033.png" alt="image-20250527154229033" style="zoom:50%;" />

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250527154300842.png" alt="image-20250527154300842" style="zoom:50%;" />

shuffle即将数据样本进行打乱操作，使每个批次内的样本分布更接近整体数据的真实分布，unshuffle 则相反

# CLIP

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250527173215418.png" alt="image-20250527173215418" style="zoom:50%;" />

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250527173244535.png" alt="image-20250527173244535" style="zoom:50%;" />

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250527173634682.png" alt="image-20250527173634682" style="zoom:50%;" />

对角线一致应该尽可能大，不在对角线上不是一张图应该较小（一个batch的图像组成矩阵）

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250601015134481.png" alt="image-20250601015134481" style="zoom:50%;" />

直接由图片生成文本难度巨大，CLIP分别将文本与图片进行编码，映射得到向量后在空间内进行匹配

CLIP摆脱了类别的限制，不需要预设最终的分类数目，可以自由应用在新的数据集上，迁移能力很强（4亿配对图片文本数据量）

训练时将类别嵌入句子中可以一定程度上提升精度

Transformer模型通常在大规模无标签文本上进行预训练，学习通用的语言表示，再在具体任务上进行微调

弱监督方式，迁移学习，泛化性能，大数据，大模型，对比学习方法

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250527230114657.png" alt="image-20250527230114657" style="zoom:50%;" />

通过可学习的矩阵将单模态的图像特征投影至多模态空间内，且最后只用接线性层投射

在数据量足够大的前提下不会出现过拟合，初始矩阵参数也不需要进行预训练

linear probe 指冻结模型参数，只修改最后一层的分类头以让模型适应特定数量的分类任务（few shot）

fine tune 则所有参数放开，针对下游任务端到端调整

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250527230955801.png" alt="image-20250527230955801" style="zoom: 33%;" />

大规模数据集、与下游任务无关的训练方式

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250528024252033.png" alt="image-20250528024252033" style="zoom: 33%;" />

# VILT

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250530003841825.png" alt="image-20250530003841825" style="zoom: 50%;" />

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250530005819482.png" alt="image-20250530005819482" style="zoom:50%;" />

# ALBEF

noisy web data 指网络爬取的文本图像资料大多带有很强的标签属性而不是进行描述

生成标签进行自训练以避免 noisy web data 的问题

align before fuse 通过CLIP同款的图像、文本向量比对实现

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250530152612801.png" alt="image-20250530152612801" style="zoom:50%;" />

# 对比学习

对比学习的关键在于如何去定义正样本与负样本，其余流程基本相似，具有极高的灵活性

对比学习属于无监督-自监督，目的是让同类的样本映射特征相近，异类样本映射后远离（前提代理任务能够分别相似与不相似的图片）

代理任务 -- 个体判别：每个个体自成一类，原个体经过旋转、裁剪等数据增强后仍属同一类（正），其它个体都属于不同类（负）

伪标签：已标注数据训练初始模型 → 预测大量无标注数据→ 高置信度预测结果作为伪标签 → 将伪标签数据加入训练集重新训练模型

端到端学习：让模型直接从原始输入数据中学习到最终输出结果，无需依赖人工设计的中间模块或分阶段处理流程

代理任务：为模型设计的中间训练任务，用于从未标注数据中自动生成监督信号（构造自监督的“伪标注场景”）（为了学习特征）

代理任务是为了生成自监督信号用于充当标签信息，目标函数用于衡量生成与标签的差异

代理任务不同于目标检测、分割等实际应用场景，一般是用于无特定目标的任务

对比学习目标函数：在高维特征空间内衡量不同样本之间的相似性，相似的尽量近（目标动态变化，取决于抽取样本）

对抗性目标函数：衡量概率分布差异，可以用于无监督数据的生成

NCE Loss（噪声对比估计损失） 是将密度估计问题转化为二分类问题的高效损失函数，通过区分数据样本与噪声样本学习数据分布

将多类别的损失函数计算（softmax）转化成一个二分类问题

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250531021926637.png" alt="image-20250531021926637" style="zoom:50%;" />

InfoNCE 在原始NCE的基础上引入互信息最大化思想，将二分类任务扩展为多分类任务

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250531113354186.png" alt="image-20250531113354186" style="zoom:50%;" />

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250531113614625.png" alt="image-20250531113614625" style="zoom:50%;" />

相比于 NCE 类别划分更多，K代表一个batch的负样本数量也远小于 softmax 代表的所有类别数量（K+1类的交叉熵）

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250531113654216.png" alt="image-20250531113654216" style="zoom:50%;" />

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250531113713235.png" alt="image-20250531113713235" style="zoom: 50%;" />

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250531113806007.png" alt="image-20250531113806007" style="zoom:50%;" />

## MOCO（**Momentum Contrast**）

动量接近于滑动平均，即当前时刻输出还取决于上一时刻输入（除当前时刻输入外）

在自然语言中embedding将语句字词拆分为单元，类似于字典查询，包含语义信息，但同样的拆分操作很难在图像领域中实现

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250530231452660.png" alt="image-20250530231452660" style="zoom: 33%;" />

对比学习方法希望编码器使原图单独成类（query），与数据增强版相近（query对应的匹配key），与其他图片远离，视作一个动态字典

key 的库应该尽可能大以便模型能够学到更通用、更本质的特征，字典应该尽可能大且具备一致性（泛化以避免模型学习到某种捷径）

对比学习的query与key编码器应该分开训练，二者共用编码器会使所有输出趋近相同，不包含特征信息（平凡解）

以前的对比学习方法主要存在两个问题：字典要足够大 + 样本的一致性问题

字典过大对运行内存的要求很高，每次都要把字典所有key遍历一遍效率也很低，字典过小整个样本库的编码器会相差很大，一致性很差

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250531144332387.png" alt="image-20250531144332387" style="zoom:50%;" />

端到端方式同时更新q与k的编码器，要求较大的字典与内存空间（SimCLR）

memory bank方式每个版本编码器更新的key较少，memory bank存储的key编码器版本可能相差很大，query的编码方式与key的编码方式也会相差很大，特征一致性很差

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250531144730440.png" alt="image-20250531144730440" style="zoom: 50%;" />

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250531145410768.png" alt="image-20250531145410768" style="zoom:50%;" />

```
# f_q, f_k: 查询编码器和键编码器网络
# queue: 动态字典队列，存储K个键特征 (维度 C x K)
# m: 动量系数 (通常0.99)
# t: 温度系数 (通常0.07-0.2)

# 初始化键编码器参数与查询编码器相同
f_k.params = f_q.params

# 数据加载循环
for x in loader:  # 加载一个batch的样本x，包含N个样本

    # 生成两种不同的随机增强视图
    x_q = aug(x)  # 查询视图 (如随机裁剪+色彩抖动)
    x_k = aug(x)  # 键视图 (独立采样的不同增强)

    # 前向传播 - 计算特征
    q = f_q.forward(x_q)  # 查询特征: [N x C] (N=batch大小, C=特征维度)
    k = f_k.forward(x_k)  # 键特征: [N x C]（256x128）
    k = k.detach()        # 断开梯度计算，防止反向传播

    # 计算正样本相似度 (logits): [N x 1]（点积）（256 x 1）
    l_pos = bmm(q.view(N, 1, C), k.view(N, C, 1))   # 重塑为 [N x C x 1]

    # 计算负样本相似度 (logits): [N x K]（256 x 65536）（字典大小）
    l_neg = mm(q.view(N, C), queue.view(C, K))  # [C x K] -> 结果 [N x K]

    # 拼接正负样本logits: [N x (1+K)]
    logits = cat([l_pos, l_neg], dim=1)  # 沿维度1拼接（256 x 65537）

    # 计算对比损失 (InfoNCE)
    labels = zeros(N)  # 标签全0 -> 正样本位置为每行的第0列
    loss = CrossEntropyLoss(logits / t, labels)  # 公式(1)

    # SGD更新查询编码器
    loss.backward()       # 反向传播计算梯度
    update(f_q.params)    # 更新f_q参数（query编码器）

    # 动量更新键编码器参数（m接近于1，大量保持key原样，少量按query更新）
    f_k.params = m * f_k.params + (1 - m) * f_q.params

    # 更新动态字典队列
    enqueue(queue, k)  # 当前批次键特征入队
    dequeue(queue)     # 淘汰最早入队的N个特征 (保持队列容量K)
```



<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250530173308077.png" alt="image-20250530173308077" style="zoom:50%;" />

每张图片对应一个128长度的向量存储在memory bank内（个体判别）

输入的一个batch作为正样本，抽取其它不同图片作为负样本，正负结合对memory bank进行调整（数据增强后可作为正样本）

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250530181111053.png" alt="image-20250530181111053" style="zoom:50%;" />

依据以往输出表示前文特征，前文特征表示Ct预测后文输出，真正后文输入对应的输出作为正样本，其它随机输入产生的输出作为负样本

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250530181530101.png" alt="image-20250530181530101" style="zoom: 67%;" />

感知某个物体存在着多种方式与视角，但应该存在某些不变的特征信息在各种视角下共享

对比学习：同个物体的不同视角都是正样本，其它物体的视角都是负样本（多模态下的对比学习）

不同的模态可能需要不同编码器，但Transformer可能同一处理不同模态数据

# DALL.E - 2

整体流程：输入文本 → CLIP编码文本text embedding为image embedding → 解码器

先由文本产生文本特征，再由文本特征产生图像特征，以图像特征生成，测试中效果更好（CLIP的反过程，称为unCLIP）

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250601020519699.png" alt="image-20250601020519699" style="zoom:50%;" />

在prior模型训练过程中，先用文本特征得到对应的图像特征，以图像特征为监督指导模型训练

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250601173117326.png" alt="image-20250601173117326" style="zoom:33%;" />

在两阶段的生成中，y表示文本，x表示生成图像，CLIP由文本x产生图像特征zi，P(x|zi,y)表示decoder结构，P(zi|y)表示prior结构

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250601173858469.png" alt="image-20250601173858469" style="zoom:50%;" />

<img src="C:\Users\11241\AppData\Roaming\Typora\typora-user-images\image-20250601173925758.png" alt="image-20250601173925758" style="zoom:50%;" />

# 视频生成

用于预训练的视频数据一般是30s以内视频，且尽量一个视频描述一个概念